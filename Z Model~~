#Ok I use the small x for x1, x2 x3 as variable for the Z-model so that it does not confused with the dataset variable. 

import pandas as pd
import os, sys
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

print("Hello World")
file_path = os.path.join(sys.path[0], "american_bankruptcy.csv")
df = pd.read_csv(file_path)
print(df)


required_columns = ["Total Assets", "Total Liabilities", "Working Capital", "Retained Earnings",
                    "EBIT", "Market Value", "Net Sales"]

#This x1,x2 shall be the model variable
df["x1"] = (df["X1"] - df["X14"]) / df["X10"]  # Working Capital / Total Assets
df["x2"] = df["X15"] / df["X10"]
df["x3"] = df["X12"] / df["X10"]
df["x4"] = df["X8"] / df["X17"]
df["x5"] = df["X9"] / df["X10"]

df["Z_Score"] = (1.2 * df["x1"]) + (1.4 * df["x2"]) + (3.3 * df["x3"]) + (0.6 * df["x4"]) + (1.0 * df["x5"])


def classify_z_score(z):
    if z > 2.99:
        return "Alive"  # Safe Zone
    elif z > 1.81:
        return "Uncertain"  # Gray Zone (Treat as "Alive" for binary classification)
    else:
        return "Failed"  # Distress Zone

df["Predicted_Status"] = df["Z_Score"].apply(classify_z_score)

# Convert 'Uncertain' to 'Alive' to maintain binary classification
df["Predicted_Status"] = df["Predicted_Status"].replace("Uncertain", "Alive")

# Ensure actual labels are formatted correctly
df["status_label"] = df["status_label"].str.lower().map({"alive": "Alive", "failed": "Failed"})

# Generate Confusion Matrix
cm = confusion_matrix(df["status_label"], df["Predicted_Status"], labels=["Alive", "Failed"])

# Accuracy Metrics
accuracy = accuracy_score(df["status_label"], df["Predicted_Status"])
report = classification_report(df["status_label"], df["Predicted_Status"])

# Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Alive", "Failed"], yticklabels=["Alive", "Failed"])
plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")
plt.title(f"Confusion Matrix\nAccuracy: {accuracy:.2%}")
plt.show()

# Print accuracy and classification report
print(f"Accuracy: {accuracy:.2%}")
print("\nClassification Report:\n", report)
